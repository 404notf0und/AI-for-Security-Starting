# Base-Learning-for-Security-Offers
## 404notfound的知识库
- [基础语言类]
  - [Java](#Java基础知识)
  - [Python](#Python基础知识)
- [四大科班基础课程类]
  - [数据结构]
  - [计算机网络]
  - [操作系统]
  - [计算机组成原理]
- [机器学习基础类]
  - [理论基础](#理论基础)
  - [算法原理](#算法原理)
  - [算法特性、优缺点]
  - [算法比较](#算法比较)
- [网络空间安全基础类]
  - [网络安全]
  - [应用安全]
  - [系统安全]
- [安全算法类]

# 基础语言类 #
## Java基础知识 ##
- [(Java中)堆和栈的区别](https://www.cnblogs.com/zyj-bozhou/p/6723863.html)<br>
联系数据结构中、操作系统中、C++中、Java中堆和栈的区别

## Python基础知识 ##
* [Python装饰器、迭代器、生成器原理及应用场景](https://www.jianshu.com/p/efaa19594cf4)<br>
* [Python进程、线程和协程的区别及应用场景](https://zhuanlan.zhihu.com/p/30980478)
* [Python扫描速度的优化到GIL锁的原理和优化](http://cenalulu.github.io/python/gil-in-python/)
* [masscan扫描和nmap扫描等端口探测的原理和方式](http://www.freebuf.com/articles/network/146087.html)<br>

# 机器学习基础类 #
## 理论基础 ##
- 怎么防止过拟合？<br>
<<<<<<< HEAD
答：a. 增加样本（data bias or small data的缘故），移除噪声。<br>
b. 减少特征，保留重要的特征（可以用PCA等对特征进行降维）。<br>
c. 对样本进行采样（类似bagging）。就是建树的时候，不是把所有的样本都作为输入，而是选择一个子集。<br>
d. 对特征进行采样。类似样本采样一样, 每次建树的时候，只对部分的特征进行切分。

- 机器学习里面的偏差和方差有什么区别？<br>
答：泛化误差可分解为偏差、方差与噪声之和。偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

- 样本类别不平衡怎么办？<br>
答：现有技术大体上有三类做法：假定正例少，负例多。第一类是直接对训练集的反类样例进行“欠采样”，即去除一个反例使得正、反例数目接近，然后再进行学习；第二类是对训练集里的正类样例进行“过采样”，即增加一些正例使得正、反例数目接近，然后再进行学习；第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将“再缩放”嵌入到其决策过程中，称为“阈值移动”，但是实际操作却可能有问题，主要因为“训练集是真实样本总体的无偏采样”这个假设往往不成立，未必能有效的基于训练集观测几率来推断出真实几率。

- [L1和L2有什么区别？](https://www.zhihu.com/question/26485586)<br>
答：L0：计算非零个数，用于产生绝对的稀疏性；L1：计算绝对值之和，用于产生稀疏性，对于large-scale的问题来说，可以减少存储空间；L2：计算平方和再开根号，能够起到正则化的作用，L2范数更多的是防止过拟合，提高模型的泛化能力，并且让优化求解变得稳定快速，这是因为加入L2范数之和，满足了强凸。<br>
=======
答：a. 增加样本（data bias or small data的缘故），移除噪声。
b. 减少特征，保留重要的特征（可以用PCA等对特征进行降维）。
c. 对样本进行采样（类似bagging）。就是建树的时候，不是把所有的样本都作为输入，而是选择一个子集。
d. 对特征进行采样。类似样本采样一样, 每次建树的时候，只对部分的特征进行切分。
- 机器学习里面的偏差和方差有什么区别
- [L1和L2有什么区别？](https://www.zhihu.com/question/26485586)<br>
答：L0：计算非零个数，用于产生绝对的稀疏性；L1：计算绝对值之和，用于产生稀疏性，对于large-scale的问题来说，可以减少存储空间；L2：计算平方和再开根号，能够起到正则化的作用，L2范数更多的是防止过拟合，提高模型的泛化能力，并且让优化求解变得稳定快速，这是因为加入L2范数之和，满足了强凸。
>>>>>>> 2d5ed8722e2c1aebff9a24cbf30d9dc4f3dfcd1b
从最优化问题解的平滑性来看，L1范数的最优解相对于L2范数要少，但其往往是最优解，而L2的解很多，但更多的倾向于某种局部解。
- [为什么L1能得到稀疏解？](https://www.zhihu.com/question/37096933)<br>
答：L1、L2两种正则能不能把最优的x变成0，取决于原先的函数在0点处的导数。如果本来导数不为0，那么施加L2正则后导数依然不为0，最优的x也不会变成0；而施加L1正则时，只要正则项的系数C，大于原先函数在0点处的导数的绝对值，x=0就会变成一个极小值点。上面只分析了一个参数x，事实上L1正则会使许多参数的最优值变成0，这样模型就稀疏了。

- [梯度消失、爆炸是怎么回事？](https://blog.csdn.net/qq_25737169/article/details/78847691)<br>
答：梯度爆炸和梯度消失根源：深度神经网络和反向传播（sigmoid激活函数）。都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题

## 算法原理 ##
- xgb的原理

## 算法比较 ##
- [RF、GBDT和XGB比较](https://zhuanlan.zhihu.com/p/34679467)
- lgb和xgb有什么区别

